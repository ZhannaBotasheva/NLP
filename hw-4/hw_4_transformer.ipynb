{"cells":[{"cell_type":"markdown","source":["## HW 4\n","Botasheva Zhanna"],"metadata":{"id":"rH6fxEOrRVne"},"id":"rH6fxEOrRVne"},{"cell_type":"markdown","source":["## Почувствуй мощь трансформеров в бою\n","\n","В качестве данных выберете возьмите датасет RuCola для русского языка https://github.com/RussianNLP/RuCoLA (в качестве train возьмите in_domain_train.csv, а в качестве теста in_domain_dev.csv)\n","\n","Разбейте in_domain_train на train и val\n","\n","Зафайнтьюньте и протестируйте RuBert или RuRoBerta на данной задаче (можно взять любую предобученную модель руберт с сайта huggingface. Например, ruBert-base/large https://huggingface.co/sberbank-ai/ruBert-base / https://huggingface.co/sberbank-ai/ruBert-large или rubert-base-cased https://huggingface.co/DeepPavlov/rubert-base-cased, ruRoberta-large https://huggingface.co/sberbank-ai/ruRoberta-large, xlm-roberta-base https://huggingface.co/xlm-roberta-base)\n","\n","\n","Возьмите RuGPT3 base или large и решите данное задание с помощью методов few-/zero-shot\n","\n","а) переберите несколько вариантов затравок\n","\n","б) протестируйте различное число few-shot примеров (0, 1, 2, 4)\n","\n","Обучите и протестируйте модель RuT5 на данной задаче (пример finetun’а можете найти здесь https://github.com/RussianNLP/RuCoLA/blob/main/baselines/finetune_t5.py)\n","\n","Сравните полученные результаты\n","\n"],"metadata":{"id":"UH5wbMfmRbX5"},"id":"UH5wbMfmRbX5"},{"cell_type":"code","execution_count":9,"id":"afc0b21d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afc0b21d","executionInfo":{"status":"ok","timestamp":1734887014448,"user_tz":-180,"elapsed":229,"user":{"displayName":"Zhanna Botasheva","userId":"00210181014318512704"}},"outputId":"d670ab3e-e81f-4747-8dd7-a3020497187b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Import is done.\n"]}],"source":["%matplotlib inline\n","import matplotlib\n","import random\n","#import wget\n","import os\n","import pathlib\n","import torch\n","import datetime\n","import time\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from IPython.display import HTML\n","from pathlib import Path\n","from enum import Enum\n","from tqdm import tqdm\n","from functools import partial\n","from shutil import rmtree\n","from razdel import tokenize\n","from datasets import Dataset\n","\n","from sklearn.metrics import (\n","    f1_score,\n","    matthews_corrcoef\n",")\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset as TorchDataset\n","from torch.utils.data import (\n","    random_split,\n","    TensorDataset,\n","    DataLoader,\n","    RandomSampler,\n","    SequentialSampler\n",")\n","from transformers import (\n","    AutoTokenizer,\n","    BertTokenizer,\n","    T5Tokenizer,\n","    AutoModelForMaskedLM,\n","    BertForSequenceClassification,\n","    T5ForConditionalGeneration,\n","    AutoModel,\n","    AutoModelForCausalLM,\n","    get_linear_schedule_with_warmup,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer\n",")\n","\n","tqdm.pandas()\n","\n","print('Import is done.')"]},{"cell_type":"markdown","id":"b46d11c8","metadata":{"id":"b46d11c8"},"source":["## Конфигурация"]},{"cell_type":"code","execution_count":null,"id":"1a344298","metadata":{"id":"1a344298","outputId":"88d81b08-2450-48cd-9f90-fb6446364874"},"outputs":[{"name":"stdout","output_type":"stream","text":["Configuration block is done.\n"]}],"source":["class Configurator:\n","    def __init__(self) -> None:\n","        # Инициализация конфигурации с основными параметрами\n","        self._params = {\n","            'random': {'seed': 42},  # Задание числа для генерации случайных чисел\n","\n","            'pretrained': {  # Параметры предобученных моделей\n","                'path': {\n","                    'bert': 'ai-forever/ruBert-base',  # Путь к модели BERT\n","                    'gpt': 'ai-forever/rugpt3large_based_on_gpt2',  # Путь к модели GPT\n","                    't5': 'ai-forever/ruT5-base',  # Путь к модели T5\n","                }\n","            },\n","\n","            'optimizer': {  # Параметры для оптимизаторов\n","                'lr': {  # Скорости обучения\n","                    'bert': 2e-5,  # Скорость обучения для BERT\n","                    't5': 2e-5  # Скорость обучения для T5\n","                },\n","                'eps': {  # Маленькое значение для комфортного обновления весов\n","                    'bert': 1e-8,  # Эпсилон для BERT\n","                    't5': 1e-8  # Эпсилон для T5\n","                }\n","            },\n","\n","            'dataset': {  # Параметры набора данных\n","                'url': {  # URL для загрузки данных\n","                    'train': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_train.csv?raw=true',  # URL обучающего набора\n","                    'test': 'https://github.com/RussianNLP/RuCoLA/blob/main/data/in_domain_dev.csv?raw=true'  # URL тестового набора\n","                },\n","                'path': {  # Локальные пути для сохраненных данных\n","                    'train': './train_dataset.csv',  # Путь для обучающего набора\n","                    'test': './test_dataset.csv'  # Путь для тестового набора\n","                },\n","                'name': {  # Имена наборов данных\n","                    'train': 'TRAIN',  # Имя обучающего набора\n","                    'test': 'TEST'  # Имя тестового набора\n","                },\n","                'usecols': [1, 2]  # Колонки, используемые для загрузки данных\n","            },\n","\n","            'train': {  # Параметры обучения\n","                'size': 0.9,  # Размер обучающей выборки (90%)\n","                'epochs': {  # Количество эпох для обучения\n","                    'bert': 1,  # Эпохи для BERT\n","                    't5': 2  # Эпохи для T5\n","                }\n","            },\n","\n","            'batch-size': 32  # Размер батча\n","        }\n","\n","    def __call__(self, *args, **kwargs):\n","        # Позволяет вызвать экземпляр Configurator как функцию для получения параметров\n","        param_name = args[0]  # Название параметра из аргументов\n","        path = param_name.split('.')  # Разделение имени параметра по точкам (например, 'pretrained.path.bert' на ['pretrained', 'path', 'bert'])\n","        return self._get_next(path, self._params, param_name)  # Запускает поиск значения параметра\n","\n","    def _get_next(self, path, params, param_name):\n","        result = None  # Инициализация результата\n","        if isinstance(params, dict) and len(path) > 0:  # Проверка, является ли params словарём и есть ли пути\n","            key = path.pop(0)  # Извлекаем первый элемент из пути\n","            if len(path) == 0:  # Если путь завершён\n","                if key in params:  # Проверяем, есть ли ключ в параметрах\n","                    result = params[key]  # Сохраняем результат\n","            else:  # Если путь не завершен, продолжаем поиск\n","                return self._get_next(path, params[key], param_name)\n","        if result is None:  # Если результат не найден\n","            print(f'Bad param name: {param_name}')  # Выводим сообщение об ошибке\n","        return result  # Возвращаем найденное значение или None\n","\n","conf = Configurator()  # Создание экземпляра класса Configurator\n","\n","print('Configuration block is done.')  # Сообщение о завершении настройки"]},{"cell_type":"markdown","id":"6a3b1e38","metadata":{"id":"6a3b1e38"},"source":["## Set random seed"]},{"cell_type":"code","execution_count":null,"id":"e6121820","metadata":{"id":"e6121820","outputId":"d09a973c-f190-4161-a18d-a17f6f73cc56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random seed is set.\n"]}],"source":["# Установка начального значения для генератора случайных чисел библиотеки random\n","random.seed(conf('random.seed'))  # Получаем значение seed из конфигурации и применяем его\n","\n","# Установка начального значения для генератора случайных чисел библиотеки numpy\n","np.random.seed(conf('random.seed'))  # Получаем значение seed из конфигурации и применяем его\n","\n","# Установка начального значения для генератора случайных чисел в библиотеке PyTorch для CPU\n","torch.manual_seed(conf('random.seed'))  # Получаем значение seed из конфигурации и применяем его\n","\n","# Установка начального значения для генератора случайных чисел в библиотеке PyTorch для GPU\n","torch.cuda.manual_seed(conf('random.seed'))  # Получаем значение seed из конфигурации и применяем его\n","\n","print('Random seed is set.')  # Сообщение о том, что начальное значение генератора случайных чисел установлено"]},{"cell_type":"markdown","id":"41acee44","metadata":{"id":"41acee44"},"source":["## Инициализация словаря"]},{"cell_type":"code","execution_count":null,"id":"534bb5fc","metadata":{"id":"534bb5fc","outputId":"0968d4be-78ea-4a3d-93bb-43eee62799c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total result is inizialized.\n"]}],"source":["# Инициализация словаря total_result для хранения результатов различных моделей\n","total_result = {\n","    'bert': None,  # Результат для модели BERT\n","    'gpt-zero-shot': None,  # Результат для GPT в режиме нулевой подстановки\n","    'gpt-few-shot-3': None,  # Результат для GPT в режиме few-shot с 3 примерами\n","    'gpt-few-shot-5': None,  # Результат для GPT в режиме few-shot с 5 примерами\n","    'gpt-few-shot-10': None,  # Результат для GPT в режиме few-shot с 10 примерами\n","    't5': None  # Результат для модели T5\n","}\n","\n","print('Total result is initialized.')  # Сообщение о том, что словарь total_result был инициализирован\n"]},{"cell_type":"markdown","id":"16aeeff9","metadata":{"id":"16aeeff9"},"source":["## Определения устройства"]},{"cell_type":"code","execution_count":3,"id":"f137c253","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f137c253","executionInfo":{"status":"ok","timestamp":1734887143294,"user_tz":-180,"elapsed":381,"user":{"displayName":"Zhanna Botasheva","userId":"00210181014318512704"}},"outputId":"51995b7b-0aa6-41b8-c8dd-2fabdff8b3ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n","Device is defined.\n"]}],"source":["# Определение устройства (GPU или CPU) в зависимости от доступности GPU\n","if torch.cuda.is_available():  # Проверяем, доступен ли GPU\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')  # Выводим количество доступных GPU\n","    print(f'We will use the GPU: {torch.cuda.get_device_name(0)}')  # Указываем имя первого доступного GPU\n","    device = torch.device('cuda')  # Устанавливаем устройство для вычислений на GPU\n","else:\n","    print('No GPU available, using the CPU instead.')  # Если GPU недоступен, выводим сообщение\n","    device = torch.device('cpu')  # Устанавливаем устройство для вычислений на CPU\n","\n","print('Device is defined.')  # Сообщение о том, что устройство для вычислений было определено"]},{"cell_type":"markdown","id":"0572c92a","metadata":{"id":"0572c92a"},"source":["## Создание модели, токенизатора, оптимизатора"]},{"cell_type":"code","execution_count":null,"id":"4a287bd1","metadata":{"id":"4a287bd1","outputId":"92e8a9f7-4982-4f0e-8aa3-3832a2d85583"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"]},{"name":"stdout","output_type":"stream","text":["Models, tokenizers, optimizers are created.\n"]}],"source":["# Создание токенизатора для модели BERT из предобученной версии\n","bert_tokenizer = BertTokenizer.from_pretrained(conf('pretrained.path.bert'))\n","\n","# Создание модели BERT для классификации последовательностей\n","bert_model = BertForSequenceClassification.from_pretrained(\n","    conf('pretrained.path.bert'),  # Загрузка предобученной модели BERT\n","    num_labels=2,  # Установка количества меток классов (например, бинарная классификация)\n","    output_attentions=False,  # Не сохранять внимательные веса\n","    output_hidden_states=False  # Не сохранять скрытые состояния\n",")\n","\n","# Создание оптимизатора AdamW для модели BERT\n","bert_optimizer = AdamW(bert_model.parameters(), lr=conf('optimizer.lr.bert'), eps=conf('optimizer.eps.bert'))\n","\n","# Создание токенизатора для модели GPT из предобученной версии\n","gpt_tokenizer = AutoTokenizer.from_pretrained(conf('pretrained.path.gpt'))\n","\n","# Создание модели GPT для генерации текста с использованием предобученной модели\n","gpt_model = AutoModelForCausalLM.from_pretrained(conf('pretrained.path.gpt'))\n","\n","# Создание токенизатора для модели T5 из предобученной версии (ускоренная версия не используется)\n","t5_tokenizer = T5Tokenizer.from_pretrained(conf('pretrained.path.t5'), use_fast=False)\n","\n","# Создание модели T5 для условной генерации текста из предобученной версии\n","t5_model = T5ForConditionalGeneration.from_pretrained(conf('pretrained.path.t5'))\n","\n","# Создание оптимизатора AdamW для модели T5\n","t5_optimizer = AdamW(t5_model.parameters(), lr=conf('optimizer.lr.t5'), eps=conf('optimizer.eps.t5'))\n","\n","print('Models, tokenizers, optimizers are created.')  # Сообщение о том, что модели, токенизаторы и оптимизаторы созданы"]},{"cell_type":"markdown","id":"fb3b4c8e","metadata":{"id":"fb3b4c8e"},"source":["## Загрузка наборов данных на диск"]},{"cell_type":"code","execution_count":null,"id":"e88e4693","metadata":{"id":"e88e4693","outputId":"7825cafc-b554-4d0a-d23b-3a22ce47c4c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset \"TRAIN\" is already downloaded.\n","Dataset \"TEST\" is already downloaded.\n","Datasets are downloaded.\n"]}],"source":["def load_dataset(url: str, path: str, name: str):\n","    # Проверка, существует ли файл по указанному пути\n","    if os.path.exists(path):\n","        print('Dataset \"' + name + '\" is already downloaded.')  # Сообщение о том, что датасет уже загружен\n","    else:\n","        wget.download(url, path)  # Загрузка датасета из указанного URL по указанному пути\n","        print('Dataset \"' + name + '\" is downloaded.')  # Сообщение о том, что датасет успешно загружен\n","\n","# Загрузка обучающего датасета\n","load_dataset(conf('dataset.url.train'), conf('dataset.path.train'), conf('dataset.name.train'))\n","\n","# Загрузка тестового датасета\n","load_dataset(conf('dataset.url.test'), conf('dataset.path.test'), conf('dataset.name.test'))\n","\n","print('Datasets are downloaded.')  # Сообщение о завершении загрузки всех датасетов"]},{"cell_type":"markdown","id":"ad31748d","metadata":{"id":"ad31748d"},"source":["## Загрузка наборов данных с диска"]},{"cell_type":"code","execution_count":null,"id":"a4e4c9b0","metadata":{"id":"a4e4c9b0","outputId":"5f9542bb-374f-4540-e5ad-250adbac8d92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframes are loaded.\n","Train dataframe size: 7082\n","Evaluate dataframe size: 787\n","Test dataframe size: 983\n"]}],"source":["# Загрузка обучающего датасета в DataFrame с выборкой определенных колонок из конфигурации\n","train_eval_dataframe = pd.read_csv(conf('dataset.path.train'), usecols=conf('dataset.usecols'))\n","\n","# Загрузка тестового датасета в DataFrame с выборкой определенных колонок из конфигурации\n","test_dataframe = pd.read_csv(conf('dataset.path.test'), usecols=conf('dataset.usecols'))\n","\n","# Получение случайной выборки индексов для валидации на основе заданного размера и случайного зерна\n","random_index = train_eval_dataframe.sample(frac=conf('train.size'), random_state=conf('random.seed')).index\n","\n","# Формирование валидационного DataFrame, исключая случайные индексы\n","eval_dataframe = train_eval_dataframe[~train_eval_dataframe.index.isin(random_index)]\n","\n","# Формирование обучающего DataFrame, включая только случайные индексы\n","train_dataframe = train_eval_dataframe[train_eval_dataframe.index.isin(random_index)]\n","\n","print('Dataframes are loaded.')  # Сообщение о завершении загрузки DataFrame\n","\n","# Вывод размеров обучающего и валидационного DataFrame\n","print(f'Train dataframe size: {len(train_dataframe)}')  # Размер обучающего DataFrame\n","print(f'Evaluate dataframe size: {len(eval_dataframe)}')  # Размер валидационного DataFrame\n","print(f'Test dataframe size: {len(test_dataframe)}')  # Размер тестового DataFrame\n"]},{"cell_type":"markdown","id":"9de202ad","metadata":{"id":"9de202ad"},"source":["## Мксимальная длина предложения\n"]},{"cell_type":"code","execution_count":null,"id":"8853c9bc","metadata":{"id":"8853c9bc","outputId":"af939125-e577-4f6c-c077-3f0976bebaf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Maximum length of sentences is defined.\n","Raw max_length is 45\n","Binary-based max_length is 64\n"]}],"source":["def define_raw_max_length_by_bert(sentences, raw_max_length):\n","    # Функция для определения сырой максимальной длины предложений на основе токенов BERT\n","    for sentence in sentences:\n","        # Кодирование предложения в токены, включая специальные токены\n","        input_ids = bert_tokenizer.encode(sentence, add_special_tokens=True)\n","        # Обновление сырой максимальной длины, если длина текущего предложения больше\n","        raw_max_length = max(raw_max_length, len(input_ids))\n","    return raw_max_length  # Возвращение максимальной длины\n","\n","def define_max_length(raw_max_length, threshold):\n","    # Функция для определения максимальной длины с учетом порога\n","    return threshold if threshold >= raw_max_length else define_max_length(raw_max_length, threshold * 2)\n","\n","# Определение сырой максимальной длины для обучающего набора предложений\n","raw_max_length = define_raw_max_length_by_bert(train_dataframe.sentence.values, 0)\n","\n","# Определение сырой максимальной длины для валидационного набора предложений, обновляя максимальную длину\n","raw_max_length = define_raw_max_length_by_bert(eval_dataframe.sentence.values, raw_max_length)\n","\n","# Определение сырой максимальной длины для тестового набора предложений, обновляя максимальную длину\n","raw_max_length = define_raw_max_length_by_bert(test_dataframe.sentence.values, raw_max_length)\n","\n","# Определение максимальной длины с начальным порогом 1\n","max_length = define_max_length(raw_max_length, 1)\n","\n","print('Maximum length of sentences is defined.')  # Сообщение о том, что максимальная длина предложений определена\n","print(f'Raw max_length is {raw_max_length}')  # Вывод сырой максимальной длины\n","print(f'Binary-based max_length is {max_length}')  # Вывод окончательной максимальной длины\n"]},{"cell_type":"markdown","id":"b6d81895","metadata":{"id":"b6d81895"},"source":["## Создание datasets & dataloaders"]},{"cell_type":"code","execution_count":null,"id":"05af8cf2","metadata":{"id":"05af8cf2","outputId":"e4b73b22-fea8-4959-88fe-7430769a3be1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive samples: 733 of 983 (74.57%)\n","BERT & T5 dataloaders are created.\n"]}],"source":["class T5TestDataset(TorchDataset):\n","    def __init__(self, text, tokenizer, length, device):\n","        # Инициализация T5TestDataset с текстом, токенизатором, максимальной длиной и устройством\n","        self._text = text.reset_index(drop=True)  # Сброс индексов текста\n","        self._tokenizer = tokenizer  # Токенизатор\n","        self._length = length  # Максимальная длина\n","        self._device = device  # Устройство (CPU/GPU)\n","\n","    def __len__(self):\n","        return self._text.shape[0]  # Количество элементов в датасете\n","\n","    def __getitem__(self, item):\n","        output = self._tokenize(self._text[item])  # Получение токенизированного текста\n","        return {k: v.reshape(-1).to(self._device) for k, v in output.items()}  # Возврат токенов на выбранном устройстве\n","\n","    def _tokenize(self, text):\n","        # Токенизация текста с возвратом тензоров\n","        return self._tokenizer(text,\n","                               return_tensors='pt',\n","                               padding='max_length',\n","                               truncation=True,\n","                               max_length=self._length)\n","\n","\n","class T5TrainDataset(TorchDataset):\n","    POS_LABEL = 'верно'  # Позитивная метка\n","    NEG_LABEL = 'неверно'  # Негативная метка\n","\n","    def __init__(self, text, label, tokenizer, length, device):\n","        # Инициализация T5TrainDataset с текстом, метками, токенизатором, максимальной длиной и устройством\n","        self._text = text.reset_index(drop=True)  # Сброс индексов текста\n","        self._label = label.reset_index(drop=True)  # Сброс индексов меток\n","        self._tokenizer = tokenizer  # Токенизатор\n","        self._length = length  # Максимальная длина\n","        self._device = device  # Устройство\n","\n","    def __len__(self):\n","        return self._label.shape[0]  # Количество элементов в датасете\n","\n","    def __getitem__(self, item):\n","        output = self._tokenize(self._text[item], self._length)  # Токенизация текста\n","        output = {k: v.reshape(-1).to(self._device) for k, v in output.items()}  # Возврат токенов на устройстве\n","\n","        # Определение метки (позитивная или негативная)\n","        label = self.POS_LABEL if self._label[item] == 1 else self.NEG_LABEL\n","        label = self._tokenize(label, length=2).input_ids.reshape(-1).to(self._device)  # Токенизация метки\n","\n","        output.update({'labels': label})  # Добавление метки в выходной словарь\n","        return output\n","\n","    def _tokenize(self, text, length):\n","        # Токенизация текста с возвратом тензоров\n","        return self._tokenizer(text,\n","                               return_tensors='pt',\n","                               padding='max_length',\n","                               truncation=True,\n","                               max_length=length)\n","\n","\n","def create_bert_dataset(sentences, acceptables, max_length):\n","    # Функция для создания датасета для BERT\n","    input_ids = []\n","    attention_masks = []\n","    for sentence in sentences:\n","        # Кодирование предложений в идентификаторы и маски для внимания\n","        encoded_dict = bert_tokenizer.encode_plus(\n","            sentence,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        input_ids.append(encoded_dict['input_ids'])  # Сохранение идентификаторов\n","        attention_masks.append(encoded_dict['attention_mask'])  # Сохранение масок внимания\n","    input_ids = torch.cat(input_ids, dim=0)  # Конкатенация идентификаторов\n","    attention_masks = torch.cat(attention_masks, dim=0)  # Конкатенация масок внимания\n","    acceptables = torch.tensor(acceptables)  # Преобразование меток в тензор\n","\n","    # Возврат TensorDataset для BERT\n","    return TensorDataset(input_ids, attention_masks, acceptables)\n","\n","# Анализирование положительных образцов в тестовом наборе\n","test_acceptable = test_dataframe.acceptable\n","print(f'Positive samples: {test_acceptable.sum()} of {len(test_acceptable)} ({100.0*test_acceptable.sum()/len(test_acceptable):.2f}%)')\n","\n","# Извлечение предложений и меток из обучающего, валидационного и тестового наборов данных\n","train_sentences = train_dataframe.sentence.values\n","train_acceptables = train_dataframe.acceptable.values\n","eval_sentences = eval_dataframe.sentence.values\n","eval_acceptables = eval_dataframe.acceptable.values\n","test_sentences = test_dataframe.sentence.values\n","test_acceptables = test_dataframe.acceptable.values\n","\n","# Создание BERT датасетов\n","bert_train_dataset = create_bert_dataset(train_sentences, train_acceptables, max_length)\n","bert_eval_dataset = create_bert_dataset(eval_sentences, eval_acceptables, max_length)\n","bert_test_dataset = create_bert_dataset(test_sentences, test_acceptables, max_length)\n","\n","# Создание T5 датасетов\n","t5_train_dataset = T5TrainDataset(train_dataframe['sentence'], train_dataframe['acceptable'], t5_tokenizer, max_length, device)\n","t5_eval_dataset = T5TrainDataset(eval_dataframe['sentence'], eval_dataframe['acceptable'], t5_tokenizer, max_length, device)\n","t5_test_dataset = T5TestDataset(test_dataframe['sentence'], t5_tokenizer, max_length, device)\n","\n","# Создание загрузчиков данных для BERT\n","bert_train_dataloader = DataLoader(\n","    bert_train_dataset,\n","    sampler=RandomSampler(bert_train_dataset),\n","    batch_size=conf('batch-size')  # Размер батча из конфигурации\n",")\n","bert_eval_dataloader = DataLoader(\n","    bert_eval_dataset,\n","    sampler=SequentialSampler(bert_eval_dataset),\n","    batch_size=conf('batch-size')  # Размер батча из конфигурации\n",")\n","bert_test_dataloader = DataLoader(\n","    bert_test_dataset,\n","    sampler=SequentialSampler(bert_test_dataset),\n","    batch_size=conf('batch-size')  # Размер батча из конфигурации\n",")\n","\n","# Создание загрузчиков данных для T5\n","t5_train_dataloader = DataLoader(t5_train_dataset, batch_size=conf('batch-size'), shuffle=True)\n","t5_eval_dataloader = DataLoader(t5_eval_dataset, batch_size=conf('batch-size'))\n","t5_test_dataloader = DataLoader(t5_test_dataset, batch_size=conf('batch-size'))\n","\n","print('BERT & T5 dataloaders are created.')  # Сообщение о том, что загрузчики данных для BERT и T5 созданы"]},{"cell_type":"markdown","id":"623ae05e","metadata":{"id":"623ae05e"},"source":["## Создание scheduler"]},{"cell_type":"code","execution_count":null,"id":"04c4a03f","metadata":{"id":"04c4a03f","outputId":"96ea3d23-a9b8-49c5-fa15-83c709de96a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT & T5 schedulers are created\n"]}],"source":["# Создание scheduler для оптимизатора BERT\n","bert_scheduler = get_linear_schedule_with_warmup(\n","    bert_optimizer,  # Оптимизатор, для которого создается планировщик\n","    num_warmup_steps=0,  # Количество шагов разминки (warmup)\n","    num_training_steps=len(bert_train_dataloader) * conf('train.epochs.bert')  # Общее количество шагов обучения\n",")\n","\n","# Создание scheduler для оптимизатора T5\n","t5_scheduler = get_linear_schedule_with_warmup(\n","    t5_optimizer,  # Оптимизатор, для которого создается планировщик\n","    num_warmup_steps=0,  # Количество шагов разминки (warmup)\n","    num_training_steps=len(t5_train_dataloader) * conf('train.epochs.t5')  # Общее количество шагов обучения\n",")\n","\n","print('BERT & T5 schedulers are created')  # Сообщение о том, что планировщики для BERT и T5 созданы"]},{"cell_type":"markdown","id":"8cbe30d1","metadata":{"id":"8cbe30d1"},"source":["## BERT training"]},{"cell_type":"code","execution_count":null,"id":"e3858886","metadata":{"id":"e3858886","outputId":"36d125f3-f207-4f25-e943-9ed42101f36b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======= Epoch 1 / 1 =======\n","\n","Training...\n","\tBatch 40 of 222, elapsed: 0:00:10\n","\tBatch 80 of 222, elapsed: 0:00:18\n","\tBatch 120 of 222, elapsed: 0:00:26\n","\tBatch 160 of 222, elapsed: 0:00:34\n","\tBatch 200 of 222, elapsed: 0:00:43\n","\tBatch 222 of 222, elapsed: 0:00:47\n","\n","\tAverage training loss: 0.5359252442916235\n","\tTraining epcoh took: 0:00:47\n","\n","Running validation...\n","\n","\tAccuracy: 0.76875\n","\tValidation loss: 0.5093031179904938\n","\tValidation took: 0:00:01\n","\n","Trainig complete!\n","Total trainig took: 0:00:49\n"]}],"source":["def flat_accuracy_bert(predictions, labels):\n","    # Функция для вычисления точности предсказаний\n","    predictions_flat = np.argmax(predictions, axis=1).flatten()  # Получаем предсказанные классы\n","    labels_flat = labels.flatten()  # Выравниваем метки\n","    return np.sum(predictions_flat == labels_flat) / len(labels_flat)  # Вычисление точности\n","\n","def format_time_bert(elapsed):\n","    # Функция для форматирования времени в удобный вид\n","    return str(datetime.timedelta(seconds=int(round(elapsed))))  # Конвертация времени в формат HH:MM:SS\n","\n","def log_step(step: int, offset: int, length: int, t0):\n","    # Функция для логирования прогресса обучения\n","    if ((step + 1) % offset == 0 and not step == 0) or (step == length - 1):\n","        elapsed = format_time_bert(time.time() - t0)  # Время, прошедшее с начала\n","        print(f'\\tBatch {step + 1} of {len(bert_train_dataloader)}, elapsed: {elapsed}')\n","\n","def extract_from_batch(batch, device) -> tuple:\n","    # Функция для извлечения данных из батча и переноса их на устройство\n","    b_input_ids = batch[0].to(device)  # Идентификаторы токенов\n","    b_input_mask = batch[1].to(device)  # Маска внимания\n","    b_labels = batch[2].to(device)  # Метки\n","    return b_input_ids, b_input_mask, b_labels  # Возврат тензоров\n","\n","bert_model.cuda()  # Перенос модели на GPU\n","\n","training_stats = []  # Начальная инициализация статистики обучения\n","total_t0 = time.time()  # Время начала тренировки\n","train_dataloader_length = len(bert_train_dataloader)  # Длина загрузчика обучающего датасета\n","val_dataloader_length = len(bert_eval_dataloader)  # Длина загрузчика валидационного датасета\n","\n","epochs = conf('train.epochs.bert')  # Получение количества эпох из конфигурации\n","for epoch_i in range(0, epochs):\n","    print(f'\\n======= Epoch {epoch_i + 1} / {epochs} =======\\n')\n","    print('Training...')\n","\n","    t0 = time.time()  # Время начала текущей эпохи\n","    total_train_loss = 0  # Суммарная потеря за эпоху\n","    bert_model.train()  # Установка модели в режим обучения\n","\n","    for step, batch in enumerate(bert_train_dataloader):\n","        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)  # Извлечение данных из батча\n","        bert_model.zero_grad()  # Обнуляем градиенты\n","\n","        # Прямой проход через модель\n","        res = bert_model(\n","            b_input_ids,\n","            token_type_ids=None,\n","            attention_mask=b_input_mask,\n","            labels=b_labels\n","        )\n","        loss = res['loss']  # Потеря на текущем батче\n","\n","        total_train_loss += loss.item()  # Суммируем потери для подсчета средней\n","        loss.backward()  # Обратный проход для вычисления градиентов\n","        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)  # Обрезаем градиенты\n","\n","        bert_optimizer.step()  # Обновление параметров оптимизатором\n","        bert_scheduler.step()  # Обновление скорости обучения\n","\n","        log_step(step, 40, train_dataloader_length, t0)  # Логирование прогресса\n","\n","    avg_train_loss = total_train_loss / train_dataloader_length  # Средняя потеря\n","    train_time = format_time_bert(time.time() - t0)  # Время, затраченное на обучение\n","    print(f'\\n\\tAverage training loss: {avg_train_loss}\\n\\tTraining epoch took: {train_time}')\n","\n","    print('\\nRunning validation...')\n","    t0 = time.time()  # Время начала валидации\n","    total_eval_accuracy = 0  # Счетчик общей точности оценки\n","    total_eval_loss = 0  # Счетчик общей потери оценки\n","\n","    bert_model.eval()  # Установка модели в режим оценки\n","\n","    for batch in bert_eval_dataloader:\n","        b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)  # Извлечение батча\n","\n","        with torch.no_grad():  # Отключение градиентов для валидации\n","            res = bert_model(\n","                b_input_ids,\n","                token_type_ids=None,\n","                attention_mask=b_input_mask,\n","                labels=b_labels\n","            )\n","        loss = res['loss']  # Потеря на валидационном наборе\n","        logits = res['logits']  # Получение предсказаний\n","\n","        total_eval_loss += loss.item()  # Суммируем потерю валидации\n","\n","        logits = logits.detach().cpu().numpy()  # Переносим предсказания на CPU для дальнейшей обработки\n","        label_ids = b_labels.to('cpu').numpy()  # Переносим метки на CPU\n","\n","        total_eval_accuracy += flat_accuracy_bert(logits, label_ids)  # Обновляем общую точность\n","\n","    avg_val_accuracy = total_eval_accuracy / val_dataloader_length  # Средняя точность валидации\n","    avg_val_loss = total_eval_loss / val_dataloader_length  # Средняя потеря валидации\n","    val_time = format_time_bert(time.time() - t0)  # Время валидации\n","    print(f'\\n\\tAccuracy: {avg_val_accuracy}')\n","    print(f'\\tValidation loss: {avg_val_loss}')\n","    print(f'\\tValidation took: {val_time}')\n","\n","print('\\nTraining complete!')  # Сообщение о завершении обучения\n","print(f'Total training took: {format_time_bert(time.time() - total_t0)}')  # Общее время обучения"]},{"cell_type":"markdown","id":"f83e68d1","metadata":{"id":"f83e68d1"},"source":["## BERT  testing"]},{"cell_type":"code","execution_count":null,"id":"530e2183","metadata":{"id":"530e2183","outputId":"68343d73-09f9-46e4-801e-45a35c1eb0fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT is tested, F1-score: 0.867\n"]}],"source":["bert_model.eval()  # Переключение модели BERT в режим оценки\n","predictions, true_labels = [], []  # Инициализация списков для предсказаний и истинных меток\n","\n","# Проход по тестовому загрузчику\n","for batch in bert_test_dataloader:\n","    b_input_ids, b_input_mask, b_labels = extract_from_batch(batch, device)  # Извлечение данных из батча\n","\n","    with torch.no_grad():  # Отключение градиентов для экономии памяти\n","        outputs = bert_model(\n","            b_input_ids,\n","            token_type_ids=None,  # Не используются токены типа\n","            attention_mask=b_input_mask  # Маска внимания\n","        )\n","    logits = outputs[0]  # Получение логитов из выхода модели\n","    logits = logits.detach().cpu().numpy()  # Перенос логитов на CPU и преобразование в numpy массив\n","    label_ids = b_labels.to('cpu').numpy()  # Перенос истинных меток на CPU и преобразование в numpy массив\n","\n","    predictions.append(logits)  # Добавление логитов в список предсказаний\n","    true_labels.append(label_ids)  # Добавление истинных меток в список\n","\n","# Конкатенация всех предсказаний и их преобразование в одномерный массив\n","flat_predictions = np.concatenate(predictions, axis=0)\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()  # Получение классов из логитов\n","flat_true_labels = np.concatenate(true_labels, axis=0)  # Конкатенация истинных меток\n","\n","# Вычисление F1\n","f1 = f1_score(flat_true_labels, flat_predictions)\n","total_result['bert'] = f1  # Сохранение F1ы в результирующий словарь\n","print(f'BERT is tested, F1-score: {f1:.3f}')  # Вывод F1\n"]},{"cell_type":"markdown","id":"514d93e8","metadata":{"id":"514d93e8"},"source":["## GPT preparation"]},{"cell_type":"code","execution_count":null,"id":"37eb6456","metadata":{"id":"37eb6456","outputId":"c820596f-f232-4ef3-88a3-323000e539c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPT preparation is done.\n"]}],"source":["def calc_gpt_loss(text):\n","    # Функция для вычисления потерь GPT модели для заданного текста\n","    inputs = gpt_tokenizer.encode(text, return_tensors='pt').reshape(-1).to(device)  # Токенизация текста\n","    with torch.no_grad():  # Отключение градиентов для экономии памяти\n","        loss = gpt_model(input_ids=inputs, labels=inputs).loss.item()  # Вычисление потерь модели\n","    return loss  # Возвращение потерь\n","\n","def shot_gpt(begin: str, text: str, positive_statement: str, negative_statement: str):\n","    # Функция для сравнения потерь позитивного и негативного утверждений\n","    positive_loss = calc_gpt_loss(' '.join([begin, text, positive_statement]))  # Потеря для позитивного утверждения\n","    negative_loss = calc_gpt_loss(' '.join([begin, text, negative_statement]))  # Потеря для негативного утверждения\n","\n","    # Возвращение 1, если потери для позитивного утверждения больше, иначе 0\n","    return 1 if positive_loss > negative_loss else 0\n","\n","gpt_model.to(device)  # Перенос модели GPT на заданное устройство (CPU/GPU)\n","\n","print('GPT preparation is done.')  # Сообщение о завершении подготовки модели GPT"]},{"cell_type":"markdown","id":"e257b644","metadata":{"id":"e257b644"},"source":["## GPT zero shot"]},{"cell_type":"code","execution_count":null,"id":"9a97d964","metadata":{"id":"9a97d964","outputId":"0103751d-ac69-4145-b4f8-e2fdce37cf61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Begin: Если ли здесь ошибка?, positive_statement: Нет., negative_statement: Есть.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["F1-score: 0.01349527665317139\n","\n","Begin: Если ли здесь ошибка?, positive_statement: Отсутствует., negative_statement: Присутствует.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["F1-score: 0.13184079601990048\n","\n","Begin: Если ли здесь ошибка?, positive_statement: Предложение правильное., negative_statement: Допущена ошибка.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["F1-score: 0.8543123543123543\n","\n","Begin: Проверь на ошибки., positive_statement: Нет, negative_statement: Есть.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["F1-score: 0.14390243902439026\n","\n","Begin: Проверь на ошибки., positive_statement: Отсутствуют., negative_statement: Присутствуют.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [00:45<00:00, 21.43it/s]"]},{"name":"stdout","output_type":"stream","text":["F1-score: 0.19437939110070257\n","\n","\n","GPT zero-shot is done, best F1-score: 0.854\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Определение задач для проверки текста на ошибки\n","tasks = [\n","    {\n","        'begin': 'Если ли здесь ошибка?',\n","        'positive_statement': 'Нет.',\n","        'negative_statement': 'Есть.'\n","    },\n","    {\n","        'begin': 'Если ли здесь ошибка?',\n","        'positive_statement': 'Отсутствует.',\n","        'negative_statement': 'Присутствует.'\n","    },\n","    {\n","        'begin': 'Если ли здесь ошибка?',\n","        'positive_statement': 'Предложение правильное.',\n","        'negative_statement': 'Допущена ошибка.'\n","    },\n","    {\n","        'begin': 'Проверь на ошибки.',\n","        'positive_statement': 'Нет',\n","        'negative_statement': 'Есть.'\n","    },\n","    {\n","        'begin': 'Проверь на ошибки.',\n","        'positive_statement': 'Отсутствуют.',\n","        'negative_statement': 'Присутствуют.'\n","    }\n","]\n","\n","best_f1 = 0.0  # Инициализация переменной для хранения лучшей F1\n","\n","# Цикл по задачам для проверки текста\n","for task in tasks:\n","    print(f'Begin: {task[\"begin\"]}, positive_statement: {task[\"positive_statement\"]}, negative_statement: {task[\"negative_statement\"]}')\n","\n","    # Определение функции для оценки текста с помощью моделирования zero-shot\n","    progress_function = lambda text: shot_gpt(task['begin'], text, task['positive_statement'], task['negative_statement'])\n","\n","    # Применение функции к каждому предложению в тестовом наборе\n","    y_pred = test_dataframe['sentence'].progress_apply(progress_function)\n","\n","    # Вычисление F1 на основе предсказаний и истинных меток\n","    f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n","\n","    # Обновление лучшей F1\n","    best_f1 = max(best_f1, f1)\n","\n","    print(f'F1-score: {f1}\\n')  # Вывод F1 для текущей задачи\n","\n","# Сохранение лучшей F1 в результатах\n","total_result['gpt-zero-shot'] = best_f1\n","print(f'\\nGPT zero-shot is done, best F1-score: {best_f1:.3f}')  # Вывод результатов"]},{"cell_type":"markdown","id":"aafd7db4","metadata":{"id":"aafd7db4"},"source":["## GPT few shot (3)"]},{"cell_type":"code","execution_count":null,"id":"d38b3bdd","metadata":{"id":"d38b3bdd","outputId":"64b89bfe-ea5a-4a15-b476-e77dc8c13518"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:11<00:00, 13.84it/s]"]},{"name":"stdout","output_type":"stream","text":["GPT few-shot(3) is done, F1-score: 0.854\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["promt = \"\"\"Проверить корректность предложения:\n","Иван вчера не позвонил. => Верно\n","Лесные запахи набегали волнами; в них смешалось дыхание можжевельника, вереска, брусники. => Верно\n","У многих туристов, кто посещают Кемер весной, есть шанс застать снег на вершине горы Тахталы и даже сочетать пляжный отдых с горнолыжным. => Неверно\n","\"\"\"\n","\n","# Применение функции для оценки текста на каждом предложении в test_dataframe\n","y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n","\n","# Вычисление F1 на основе предсказаний и истинных меток\n","f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n","\n","# Сохранение F1 в результирующий словарь\n","total_result['gpt-few-shot-3'] = f1\n","print(f'GPT few-shot(3) is done, F1-score: {f1:.3f}')  # Вывод результатов\n"]},{"cell_type":"markdown","id":"90b8e63b","metadata":{"id":"90b8e63b"},"source":["## GPT few shot (5)"]},{"cell_type":"code","execution_count":null,"id":"2b5dfffb","metadata":{"id":"2b5dfffb","outputId":"8a57acf6-808d-4209-ce57-383e8484508c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:11<00:00, 13.80it/s]"]},{"name":"stdout","output_type":"stream","text":["GPT few-shot(5) is done, F1-score: 0.854\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["promt = \"\"\"Проверить корректность предложения:\n","Вчера президент имел неофициальную беседу с английским послом. => Верно\n","А ты ехай прямо к директору театров, князю Гагарину. => Неверно\n","Коллега так и не признал вину за катастрофу перед коллективом. => Верно\n","Малыш уже уверенно читает слова через мягкий знак. => Неверно\n","Я говорил с ним только ради Вас. => Верно\n","\"\"\"\n","\n","# Применение функции для оценки текста на каждом предложении в test_dataframe\n","y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n","\n","# Вычисление F1 на основе предсказаний и истинных меток\n","f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n","\n","# Сохранение F1 в результирующий словарь\n","total_result['gpt-few-shot-5'] = f1\n","print(f'GPT few-shot(5) is done, F1-score: {f1:.3f}')  # Вывод результатов"]},{"cell_type":"markdown","id":"117a5550","metadata":{"id":"117a5550"},"source":["## GPT few shot (10)"]},{"cell_type":"code","execution_count":null,"id":"1c391f41","metadata":{"id":"1c391f41","outputId":"13c5558d-86ac-4cc1-b46a-89d346db1f7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 983/983 [01:48<00:00,  9.05it/s]"]},{"name":"stdout","output_type":"stream","text":["GPT few-shot(10) is done, F1-score: 0.854\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["promt = \"\"\"Проверить корректность предложения:\n","Только бы он громко не закричал, когда найдет решение. => Верно\n","Но Коле не помог его иноверец. => Неверно\n","Гармоничные пропорции здания основаны на классических образцах. => Верно\n","Вчера мне нужно ехать на завод. => Неверно\n","Дело приняло дурной оборот. => Верно\n","Я стою поблизости от Центризбиркома и наблюдаю за тем, что происходит у входа. => Неверно\n","Вани не было в школе. => Верно\n","В качестве примеров были приведены случаи, о которых, кажется, что где-то я уже читал. => Неверно\n","Кожа у виска была желтой. => Верно\n","В эту минуту роман был прочитан. => Неверно\n","\"\"\"\n","\n","# Применение функции для оценки текста на каждом предложении в test_dataframe\n","y_pred = test_dataframe['sentence'].progress_apply(lambda text: shot_gpt(promt, text, ' => Верно', ' => Неверно'))\n","\n","# Вычисление F1 на основе предсказаний и истинных меток\n","f1 = f1_score(y_pred, test_dataframe[\"acceptable\"])\n","\n","# Сохранение F1 в результирующий словарь\n","total_result['gpt-few-shot-10'] = f1\n","print(f'GPT few-shot(10) is done, F1-score: {f1:.3f}')  # Вывод результатов\n"]},{"cell_type":"markdown","id":"afd06a93","metadata":{"id":"afd06a93"},"source":["## T5 training"]},{"cell_type":"code","execution_count":null,"id":"60d1cc0f","metadata":{"id":"60d1cc0f","outputId":"4f447ef9-9781-4044-b290-f62b22d17aaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======= Epoch 1 / 2 =======\n","\n","Training...\n","\n","\t\tBatch 10 of 222, loss : 5.728\n","\t\tBatch 20 of 222, loss : 2.136\n","\t\tBatch 30 of 222, loss : 1.363\n","\t\tBatch 40 of 222, loss : 0.833\n","\t\tBatch 50 of 222, loss : 0.582\n","\t\tBatch 60 of 222, loss : 0.552\n","\t\tBatch 70 of 222, loss : 0.529\n","\t\tBatch 80 of 222, loss : 0.374\n","\t\tBatch 90 of 222, loss : 0.644\n","\t\tBatch 100 of 222, loss : 0.402\n","\t\tBatch 110 of 222, loss : 0.395\n","\t\tBatch 120 of 222, loss : 0.370\n","\t\tBatch 130 of 222, loss : 0.294\n","\t\tBatch 140 of 222, loss : 0.499\n","\t\tBatch 150 of 222, loss : 0.300\n","\t\tBatch 160 of 222, loss : 0.430\n","\t\tBatch 170 of 222, loss : 0.437\n","\t\tBatch 180 of 222, loss : 0.233\n","\t\tBatch 190 of 222, loss : 0.542\n","\t\tBatch 200 of 222, loss : 0.252\n","\t\tBatch 210 of 222, loss : 0.401\n","\t\tBatch 220 of 222, loss : 0.194\n","\t\tBatch 222 of 222, loss : 0.191\n","Validation...\n","\tValidation loss: 0.2917381364107132\n","\n","======= Epoch 2 / 2 =======\n","\n","Training...\n","\n","\t\tBatch 10 of 222, loss : 0.354\n","\t\tBatch 20 of 222, loss : 0.500\n","\t\tBatch 30 of 222, loss : 0.333\n","\t\tBatch 40 of 222, loss : 0.327\n","\t\tBatch 50 of 222, loss : 0.284\n","\t\tBatch 60 of 222, loss : 0.403\n","\t\tBatch 70 of 222, loss : 0.352\n","\t\tBatch 80 of 222, loss : 0.358\n","\t\tBatch 90 of 222, loss : 0.466\n","\t\tBatch 100 of 222, loss : 0.378\n","\t\tBatch 110 of 222, loss : 0.410\n","\t\tBatch 120 of 222, loss : 0.325\n","\t\tBatch 130 of 222, loss : 0.271\n","\t\tBatch 140 of 222, loss : 0.211\n","\t\tBatch 150 of 222, loss : 0.287\n","\t\tBatch 160 of 222, loss : 0.307\n","\t\tBatch 170 of 222, loss : 0.427\n","\t\tBatch 180 of 222, loss : 0.253\n","\t\tBatch 190 of 222, loss : 0.376\n","\t\tBatch 200 of 222, loss : 0.354\n","\t\tBatch 210 of 222, loss : 0.290\n","\t\tBatch 220 of 222, loss : 0.393\n","\t\tBatch 222 of 222, loss : 0.230\n","Validation...\n","\tValidation loss: 0.290314502120018\n","Trainig complete!\n"]}],"source":["t5_model.to(device)  # Перемещение модели T5 на заданное устройство (CPU или GPU)\n","n_epochs = conf('train.epochs.t5')  # Получение количества эпох из конфигурационного файла\n","\n","dl_length = len(t5_train_dataloader)  # Длина обучающего загрузчика\n","for epoch in range(n_epochs):  # Цикл по количеству эпох\n","    print(f'\\n======= Epoch {epoch + 1} / {n_epochs} =======\\n')\n","\n","    print('Training...\\n')\n","    t5_model.train()  # Установка модели в режим обучения\n","\n","    for batch_id, batch in enumerate(t5_train_dataloader):  # Итерация по батчам\n","        outputs = t5_model(**batch)  # Прямой проход через модель\n","        loss = outputs.loss  # Получение потерь\n","\n","        loss.backward()  # Обратное распространение градиента\n","        t5_optimizer.step()  # Обновление параметров модели\n","        t5_scheduler.step()  # Обновление скорости обучения\n","        t5_optimizer.zero_grad()  # Обнуление градиентов для следующего батча\n","\n","        # Логирование потерь каждые 10 батчей и на последнем батче\n","        if (((batch_id + 1) % 10 == 0) and not batch_id == 0) or (batch_id == dl_length - 1):\n","            print(f'\\t\\tBatch {batch_id + 1} of {dl_length}, loss : {loss.item():.3f}')\n","\n","    print('Validation...')\n","    t5_model.eval()  # Установка модели в режим оценки\n","\n","    with torch.no_grad():  # Отключение градиентов для экономии памяти\n","        eval_loss = [t5_model(**batch).loss.item() for batch in t5_eval_dataloader]  # Получение потерь для валидации\n","\n","    print(f'\\tValidation loss: {np.sum(eval_loss) / len(eval_loss)}')  # Вывод средней потери на валидации\n","\n","print('Training complete!')  # Сообщение о завершении обучения"]},{"cell_type":"markdown","id":"ac17a694","metadata":{"id":"ac17a694"},"source":["## T5 testing"]},{"cell_type":"code","execution_count":null,"id":"c0ff86b8","metadata":{"id":"c0ff86b8","outputId":"b9ae1bfb-311a-44f4-b06c-5b4ee78d6400"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["T5 testing is done, F1-score: 0.854\n"]}],"source":["# Токенизация положительного ярлыка\n","pos_label = t5_tokenizer(T5TrainDataset.POS_LABEL,\n","                         return_tensors='pt',\n","                         padding='max_length',\n","                         truncation=True,\n","                         max_length=2)['input_ids'][0][0].item()  # Получение токена для положительного ярлыка\n","\n","t5_model.eval()  # Переключение модели T5 в режим оценки\n","\n","result = np.array([])  # Инициализация массива для хранения результатов\n","for batch in t5_test_dataloader:  # Итерация по тестовому загрузчику\n","    tokens = t5_model.generate(**batch)  # Генерация токенов с помощью модели T5\n","    # Подсчет наличия положительного ярлыка в сгенерированных токенах\n","    tokens = [1 if pos_label in token else 0 for token in tokens]\n","    result = np.hstack([result, tokens])  # Объединение результатов с ранее полученными\n","\n","# Вычисление F1 на основе предсказанных и истинных меток\n","f1 = f1_score(result, test_dataframe[\"acceptable\"])\n","total_result['t5'] = f1  # Сохранение F1 в результирующий словарь\n","\n","print(f'T5 testing is done, F1-score: {f1:.3f}')  # Вывод результатов"]},{"cell_type":"markdown","id":"b2291480","metadata":{"id":"b2291480"},"source":["## Результаты\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a4632547","metadata":{"id":"a4632547","outputId":"5c3696d0-cf07-4343-daf7-33b067f30480"},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Type</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>bert</td>\n","      <td>0.867310</td>\n","    </tr>\n","    <tr>\n","      <td>gpt-zero-shot</td>\n","      <td>0.854312</td>\n","    </tr>\n","    <tr>\n","      <td>gpt-few-shot-3</td>\n","      <td>0.854312</td>\n","    </tr>\n","    <tr>\n","      <td>gpt-few-shot-5</td>\n","      <td>0.854312</td>\n","    </tr>\n","    <tr>\n","      <td>gpt-few-shot-10</td>\n","      <td>0.854312</td>\n","    </tr>\n","    <tr>\n","      <td>t5</td>\n","      <td>0.854142</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Создание списка данных из словаря total_result\n","data = [[k, v] for k, v in total_result.items()]  # Генерация списка пар (тип модели, F1)\n","\n","# Создание DataFrame на основе полученных данных\n","df = pd.DataFrame(data, columns=['Type', 'F1'])  # Указание названий столбцов\n","\n","# Отображение DataFrame в HTML формате без индексов\n","HTML(df.to_html(index=False))  # Преобразование DataFrame в HTML и визуализация"]},{"cell_type":"markdown","source":["BERT является самой эффективной моделью для текущих задач обработки текста, в то время как различные варианты GPT демонстрируют схожую эффективность. Модель T5 выступает немного ниже, но также имеет неплохие результаты."],"metadata":{"id":"YwDkYzI1uxXR"},"id":"YwDkYzI1uxXR"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}